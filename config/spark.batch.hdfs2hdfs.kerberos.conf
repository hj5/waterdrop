######
###### This config file is a demonstration of batch processing in waterdrop config
######

env {
  # You can set spark configuration here
  # see available properties defined by spark: https://spark.apache.org/docs/latest/configuration.html#available-properties
  spark.app.name = "Waterdrop-hive2"
  spark.executor.instances = 1
  spark.executor.cores = 1
  spark.executor.memory = "1g"
  #spark.sql.catalogImplementation = "hive"
  #spark.sql.catalogImplementation = "in-memory"
  #kerberos认证配置：
  spark.hadoop.hadoop.security.authentication = "kerberos"
  spark.hadoop.hadoop.security.authorization = "true"
  spark.hadoop.hadoop.rpc.protection = "privacy"
  spark.hadoop.dfs.namenode.kerberos.principal = "hdfs/hadoop.hadoop.com@HADOOP.COM"
  spark.hadoop.dfs.datanode.kerberos.https.principal="hdfs/hadoop.hadoop.com@HADOOP.COM"
  spark.hadoop.dfs.datanode.kerberos.principal = "hdfs/hadoop.hadoop.com@HADOOP.COM"
  #Insecure To Secure跨集群配置：
  spark.hadoop.ipc.client.fallback-to-simple-auth-allowed = "true"
}

source {
  # This is a example input plugin **only for test and demonstrate the feature input plugin**
  # Fake {
  #   result_table_name = "my_dataset"
  # }

  #hive
  #hive {
  #  pre_sql = "select * from alex1 where name='"${city}"'"
  #  result_table_name = "alex1"
  #}

  # You can also use other input plugins, such as hdfs
  # hdfs {
  #   result_table_name = "accesslog"
  #   path = "hdfs://hadoop-cluster-01/nginx/accesslog"
  #   format = "json"
  # }
  hdfs {
       result_table_name = "accesslog"
       path = "hdfs://bfd-haifeng--1.novalocal:8020/tmp/flink/hello.txt"
       format = "text"
  }
  #hdfs {
  #       result_table_name = "accesslog"
  #       path = "hdfs://172.24.3.173:25006/tmp/abc/aa.txt"
  #       format = "text"
  #       kerberos.keytab = "/Users/hj/Documents/workspace/bdos/waterdrop/plugin-spark-sink-file/src/main/resources/presto.keytab"
  #       kerberos.username = "api/presto-server"
  #       #options.spark.hadoop.fs.defaultFS = "hdfs://172.24.3.173:25006"
  #}

  # If you would like to get more information about how to configure waterdrop and see full list of input plugins,
  # please go to https://interestinglab.github.io/waterdrop/#/zh-cn/configuration/base
}

transform {
  # split data by specific delimiter

  # you can also you other filter plugins, such as sql
  # sql {
  #   sql = "select * from accesslog where request_time > 1000"
  # }

  # If you would like to get more information about how to configure waterdrop and see full list of filter plugins,
  # please go to https://interestinglab.github.io/waterdrop/#/zh-cn/configuration/base
}

sink {
  # choose stdout output plugin to output data to console
  Console {
  #  soure_table_name = "alex1"
  }

  #elasticsearch {
  #    hosts = ["172.24.5.212:9200"]
  #    #index = "waterdrop-${now}"
  #    index = "waterdrop"
  #    es.mapping.id = "id"
  #    es.batch.size.entries = 100000
  #    index_time_format = "yyyy.MM.dd"
  #}



  # you can also you other output plugins, such as sql
  # hdfs {
  #   path = "hdfs://hadoop-cluster-01/nginx/accesslog_processed"
  #   save_mode = "append"
  # }
  hdfs {
       path = "hdfs://172.24.3.173:25006/tmp/hj/out"
       save_mode = "append",
       serializer = "text"
       partition_by = []
       path_time_format = "yyyyMMddHHmmss"
       kerberos.username = "api/presto-server"
       kerberos.keytab = "/Users/hj/Documents/workspace/bdos/waterdrop/plugin-spark-sink-file/src/main/resources/presto.keytab"
  }

  # If you would like to get more information about how to configure waterdrop and see full list of output plugins,
  # please go to https://interestinglab.github.io/waterdrop/#/zh-cn/configuration/base
}
